%! Author = Stepan Oskin
%! Date = 2019-08-01

% Preamble
\documentclass[11pt]{article}

% Packages

% Document
\begin{document}

    \title{OSEMN methodology \\
    Step 2: Scrub (clean) data \\
    Cleanup plan for Teranet dataset}

    \author{Stepan Oskin}

    \maketitle

    \begin{abstract}
        This document outlines the cleanup plan for the Teranet dataset.
        The four main steps of the cleanup process include: spatial join of Teranet points with Dissemination Area polygons, correction of inconsistent entries, addition of new attributes, and removal of duplicated transactions.
        Additional steps may include: filtering transactions for low and high outliers based on the values of \texttt{consideration\_amt}, and correction of \texttt{consideration\_amt} for inflation to ensure consistency of dollar values across time.
    \end{abstract}

    \section{Introduction} \label{sec:teranet_cleaning_intro}

    For description of OSEMN methodology for data science projects, see \\ \texttt{methodology/0.osemn/osemn.pdf}.

    \vspace{5mm}

    For background information, description of the Teranet dataset, and its attributes, see \texttt{methodology/1.obtain/obtain.pdf}.

    \vspace{5mm}

    As outlined in \textit{Step 2: Scrub (clean) data} of OSEMN methodology (see \texttt{methodology/2.scrub/scrub.pdf} for details), data analytics, statistical models, and machine learning algorithms, in order to produce meaningful results, require input data to be consistent and transformed into appropriate form.

    In addition to that, as outlined in the \textit{Description of Relational Databases} (see \texttt{methodology/rdbms/rdbms.pdf} for details), the main idea of a database is to force the data into a certain structure to allow implementation of constraints and relationships between entities.
    Organizing data in such a way allows for reduction of redundancy in data storage and improvement of efficiency in working with related datasets.
    Thus, input data for a relational database also needs to be consistent in order for constraints and relations to work properly.

    Data also needs to follow the ''tidy'' format, as described in section~\ref{sec:db_norm_tidy_data}.
    Teranet dataset is tidy in its structure, but does have multiple issues with its consistency, such as inconsistent spelling, missing and erratic values, and duplicate records.
    This document describes the steps taken to address these issues and improve the quality and usability of Teranet dataset, and to meet the requirements needed for the organization of the proposed Teranet database.

    \section{Cleanup plan for the Teranet dataset} \label{sec:teranet_cleanup_plan}

    The four main steps of the cleanup process of the Teranet dataset include:
    \begin{itemize}
        \item Step 2.1: spatial join of Teranet points with Dissemination Area (DA) polygons
        \item Step 2.2: correction of inconsistent entries
        \item Step 2.3: addition of new attributes: \texttt{year}, \texttt{decade}, and \texttt{transaction\_id}
        \item Step 2.4: removal of duplicate records from the Teranet dataset
    \end{itemize}

    \vspace{5mm}

    Additional cleanup steps may include:
    \begin{itemize}
        \item filtering Teranet transactions for low and high outliers based on the values of \texttt{consideration\_amt}
        \item correction of \texttt{consideration\_amt} for inflation to ensure consistency of dollar values across time
    \end{itemize}

    \section{Tidy data and database normalization} \label{sec:db_norm_tidy_data}

    This section describes the concept of ''Tidy Data'', as defined by Hadley Wickham.
    The concept of ''Tidy Data'' presents the basic ideas of normalization of a database, as defined by Edgar F. Codd, reformulated in statistical language.

    \subsection{Tidy data} \label{subsec:tidy_data}

    Hadley Wickham in his paper \textit{''Tidy Data''}\cite{Wickham2014} formalized the way how a shape of the data can be described and what goal should be pursued when formatting data.
    The principles of tidy data provide a standard way to organize data values within a dataset.
    The tidy data standard has been designed to facilitate initial exploration and analysis of the data, and to simplify the development of data analysis tools that work well together.
    The principles of tidy data are closely tied to those of relational databases and Codd's relational algebra\cite{Codd1990}.

    As an integral part of his \textbf{relational model}, Codd proposed a process of database normalization, or restructuring of a relational database in accordance with a series of so-called \textbf{normal forms} in order to reduce data redundancy and improve data integrity.

    \subsection{Normalization of a database according to Codd} \label{subsec:db_norm}

    Normalization entails organizing the columns (attributes) and tables (relations) of a database to ensure that their dependencies are properly enforced by database integrity constraints.
    As defined by Codd (\cite{Codd1990}, section 17.5.1), the basic ideas in normalization are to organize the information in a database as follows:

    \begin{enumerate}
        \item Each distinct type of object has a distinct type identifier, which becomes the name of a base relation.
        \item Every distinct object of a given type must have an instance identifier that is unique within the object type;
        this is called its \textbf{primary-key} value.
        \item Every fact in the database is a fact about the object identified by the primary key.
        \item Each such fact contains nothing other than the single-valued immediate properties of the object.
        \item Such facts are collected together in a single relation, if they are about objects of the same type.
        The result is a collection of facts, all of the same type.
    \end{enumerate}

    \subsection{Teranet dataset in the context of a normalized database} \label{subsec:teranet_db_norm}

    Teranet dataset is intended to be used as one of the tables of the proposed housing database that would include other sources of information, such as neighbourhood demographics, building types and coordinates, \textit{etc.}
    In this context, Codd's basic normalization ideas would take the following form:

    \begin{enumerate}
        \item The Teranet dataset presents a single type of object (relation, or table) \textemdash real estate transactions recorded in the Province of Ontario between 1805-01-06 and 2017-10-11.
        \item Every distinct object (transaction) must have an instance identifier that is unique within the object type, or its primary-key value.
        In case of Teranet dataset, all the native columns, including \texttt{registration\_date}, \texttt{pin}, address information, and \texttt{X} and \texttt{Y} coordinates, have duplicated values present (multiple transactions occurring on the same date, address, coordinates, or under the same pin.
        Thus, no combination of such columns constitute a candidate key, which prompts the addition of a new column that can serve as a unique identifier for Teranet records.
        \item A new column \texttt{transaction\_id} is added to the dataset to be used as its primary key;
        it corresponds to the row number of each instance (transaction) in the Teranet dataset (filtered to include only GTHA transactions via a spatial join, see section~\ref{sec:teranet_da_spatial_join}), ordered from the earliest date to the latest.
        \item Every fact in the database is a fact about the object identified by the primary key.
        This condition is met, as every transaction in Teranet dataset is described by the values found in columns of a single row.
        \item Each such fact contains nothing other than the single-valued immediate properties of the object, all columns in Teranet dataset contain single-valued immediate properties of each transaction.
        \item Such facts are collected together in a single relation, as they are all objects of the same type (a single table of real estate transactions recorded in Ontario).
    \end{enumerate}

    Thus, Teranet dataset fits into a normalized database, with the new attribute \texttt{transaction\_id} as its primary key.

    \subsection{Codd's constraints formed in statistical language by Wickham} \label{subsec:teranet_tidy_data}

    According to Wickham\cite{Wickham2014}, \textit{tidy data} is a standard way of mapping the meaning of a dataset to its structure.
    A dataset is messy or tidy depending on how rows, columns and tables are matched up with observations,
    variables and types.

    \vspace{5mm}

    In tidy data:
    \begin{enumerate}
        \item Each variable forms a column.
        \item Each observation forms a row.
        \item Each type of observational unit forms a table.
    \end{enumerate}

    This is Codd's 3rd normal form\cite{Codd1990}, but with the constraints framed in statistical
    language, and the focus put on a single dataset rather than the many connected datasets common in relational databases.
    \textit{Messy data} is any other arrangement of the data.

    \vspace{5mm}

    According to Wickham, the most common problems with messy datasets are:
    \begin{itemize}
        \item Column headers are values, not variable names.
        \item Multiple variables are stored in one column.
        \item Variables are stored in both rows and columns.
        \item Multiple types of observational units are stored in the same table.
        \item A single observational unit is stored in multiple tables.
    \end{itemize}

    In the Teranet dataset, none of these problems are present, so it presents \textit{tidy data}.

    \section{Step 2.1: Spatial join of Teranet points with Dissemination Area polygons} \label{sec:teranet_da_spatial_join}

    Step 2.1 of the cleaning process of Teranet data involved the spatial join of Teranet points with Dissemination Area (DA) polygons.
    Parameters that were used for the spatial join operation were \texttt{how='inner', op='within'}.

    The spatial join was performed to filter out Teranet records whose coordinates fall outside of GTHA .

    In addition to that, three new attributes were produced as a results of the spatial join:
    \begin{itemize}
        \item Dissemination Area attributes \texttt{OBJECTID}, \texttt{DAUID}, and \texttt{CSDNAME} were added to each Teranet record falling within a particular DA polygon.
        \item The added attributes allow for extra quality control of Teranet data by comparing the column \texttt{MUNICIPALITY} of Teranet records with the column \texttt{CSDNAME} associated with DA geometry.
        \item New columns \texttt{OBJECTID} and \texttt{DAUID} allow Teranet records to be joined in the future with DA geometry via a regular (non-spatial) join operation (for example, to be aggregated by DAs), or to add any additional DA-level attributes, such as DA-level demographics, to Teranet records.
        \item These future joins can be performed via regular (non-spatial) join operations, which are much less computationally intensive than a spatial join, and thus can be performed much faster.
    \end{itemize}

    \section{Step 2.2: Correction of inconsistent entries} \label{sec:teranet_correction_inconsistent}

    Step 2.2 of the cleanup process for Teranet data focused on the correction of inconsistent entries.

    \vspace{5mm}

    The following modifications have been made to the dataset:

    Inconsistent capitalizations were fixed for
    \begin{itemize}
        \item column names
        \item column \texttt{municipality}
        \item \texttt{street\_name}
        \item \texttt{street\_designation}
        \item values in the column \texttt{postal\_code} did not show any problems, but were converted to upper case as a preventive measure
    \end{itemize}

    \section{Step 2.3: Addition of new attributes} \label{sec:teranet_new_cols}

    Step 2.3 of the cleanup process for Teranet dataset focused on the addition of three more new attributes to each Teranet record (in addition to the three DA-level attributes that were added in Step 2.1).

    \vspace{5mm}

    Two new attributes parsed from \texttt{registration\_date} for grouping and visualization purposes
    \begin{itemize}
        \item \texttt{year}
        \item \texttt{decade}
    \end{itemize}

    \vspace{5mm}

    A new attribute \texttt{transaction\_id} was added to provide unique identifier for Teranet records:
    \begin{itemize}
        \item No combination of native columns provides a convenient unique identification for Teranet records
        \item \texttt{pin}, \texttt{registration\_date}, \texttt{consideration\_amt}, \texttt{X} and \texttt{Y} coordinates, and other columns all have duplicate entries, and thus do not qualify as candidate keys
        \item A new column \texttt{transaction\_id} with a simple auto-incrementing range index (row number) will be added to provide a unique identifier for each Teranet record
        \item The new column \texttt{transaction\_id} can be used as a Primary Key for Teranet table within the proposed Teranet database, and thus allows to take advantage of the indexing features available in PostgreSQL
    \end{itemize}

    \section{Step 2.4: Removal of duplicate records} \label{sec:teranet_drop_duplicates}

    \medskip

    \bibliography{teranet_cleanup_plan}
    \bibliographystyle{ieeetr}

\end{document}